{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRIDUL HARISH, CED18I034, PROBLEM SET 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import re\n",
    "import itertools\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_df = pd.read_csv(\"Market_Basket_Optimisation.csv\",header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 : Extend the Apriori Algorithm discussed in the class supporting Transaction Reduction approach to improve the time complexity issue as a result of the repeated scans limitation of Apriori. You may compare this extended version with the earlier implementations in (1) over the same benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample :  ['shrimp', 'almonds', 'avocado', 'vegetables mix', 'green grapes', 'whole weat flour', 'yams', 'cottage cheese', 'energy drink', 'tomato juice', 'low fat yogurt', 'green tea', 'honey', 'salad', 'mineral water', 'salmon', 'antioxydant juice', 'frozen smoothie', 'spinach', 'olive oil']\n"
     ]
    }
   ],
   "source": [
    "records = [[y for y in x if pd.notna(y)] for x in market_df.values.tolist()]\n",
    "print(\"Sample : \",  records[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'shrimp'})  =  536\n",
      "frozenset({'almonds'})  =  153\n",
      "frozenset({'avocado'})  =  250\n",
      "frozenset({'vegetables mix'})  =  193\n",
      "frozenset({'green grapes'})  =  68\n",
      "frozenset({'whole weat flour'})  =  70\n",
      "frozenset({'yams'})  =  86\n",
      "frozenset({'cottage cheese'})  =  239\n",
      "frozenset({'energy drink'})  =  200\n",
      "frozenset({'tomato juice'})  =  228\n",
      "frozenset({'low fat yogurt'})  =  574\n"
     ]
    }
   ],
   "source": [
    "Database = {}\n",
    "for i in range(len(records)):\n",
    "  Database[\"T\" + str(i+1)] = records[i]\n",
    "\n",
    "Itemset = {}\n",
    "for i in range(len(records)):\n",
    "  for j in range(len(records[i])):\n",
    "    if(frozenset([records[i][j]]) not in Itemset):\n",
    "      Itemset[frozenset([records[i][j]])] = 1\n",
    "    else:\n",
    "      Itemset[frozenset([records[i][j]])] += 1\n",
    "\n",
    "def get_items(Itemset,no_of_items,cur):\n",
    "  for key,val in Itemset.items():\n",
    "    print(key,\" \",val)\n",
    "\n",
    "def check(miniset,Database):\n",
    "  count = 0\n",
    "  for key,val in Database.items():\n",
    "    if(frozenset(val).intersection(miniset) == miniset):\n",
    "      count+=1\n",
    "  return count\n",
    "\n",
    "def get_c(Li,Database,itert):\n",
    "  c={}\n",
    "  for key,vals in Li.items():\n",
    "    for key1,vals1 in Li.items():\n",
    "      if(key1!=key):\n",
    "        miniset = key1.union(key)\n",
    "        if(len(miniset)>itert):\n",
    "          continue\n",
    "        count = check(miniset,Database)\n",
    "        c[miniset] = count\n",
    "  return c\n",
    "\n",
    "def remove_transaction(Database,sot):\n",
    "  rem_keys = []\n",
    "  for key,val in Database.items():\n",
    "    if(len(val) <= sot):\n",
    "      rem_keys.append(key)\n",
    "  \n",
    "  for key in rem_keys:\n",
    "    Database.pop(key)\n",
    "  return Database\n",
    "\n",
    "def remove_items(c,min_sup_count):\n",
    "  rem_keys = []\n",
    "  for key,val in c.items():\n",
    "    if(val < min_sup_count):\n",
    "      rem_keys.append(key)\n",
    "  \n",
    "  for key in rem_keys:\n",
    "    c.pop(key)\n",
    "  \n",
    "  return c\n",
    "\n",
    "def remove_item(Li,Database):\n",
    "  miniset = set()\n",
    "  for key,val in Li.items():\n",
    "    miniset = miniset.union(key)\n",
    "  for key,val in Database.items():\n",
    "    Database[key] = list(set(val) & miniset)\n",
    "  \n",
    "  return Database\n",
    "\n",
    "min_sup_count = 0.005*len(records)\n",
    "Li = {}\n",
    "for key,val in Itemset.items():\n",
    "  if(val >= min_sup_count):\n",
    "    Li[key] = val\n",
    "\n",
    "Itemset = Li\n",
    "\n",
    "countitem = 0\n",
    "for key,val in Itemset.items():\n",
    "  print(key,\" = \",val)\n",
    "  countitem += 1\n",
    "  if(countitem>10):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Num  1\n",
      "Iteration Num  2\n",
      "Iteration Num  3\n",
      "\n",
      "Time taken =  146.109375 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "Final_List = []\n",
    "sot=1\n",
    "final_c={}\n",
    "while(1):\n",
    "  print(\"Iteration Num \",sot)\n",
    "  Database = remove_transaction(Database,sot)\n",
    "  c = get_c(Li,Database,sot+1)\n",
    "  sot+=1\n",
    "\n",
    "  Li = remove_items(c,min_sup_count)\n",
    "  Database = remove_item(Li,Database)\n",
    "  if(len(Li) == 0):\n",
    "    break\n",
    "  else:\n",
    "    final_c=Li\n",
    "\n",
    "time_taken = time.process_time() - start\n",
    "print(\"\\nTime taken = \",str(time_taken),\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'mineral water', 'shrimp', 'eggs'})  =  39\n",
      "frozenset({'mineral water', 'shrimp', 'milk'})  =  59\n",
      "frozenset({'mineral water', 'frozen vegetables', 'shrimp'})  =  54\n",
      "frozenset({'mineral water', 'spaghetti', 'shrimp'})  =  64\n",
      "frozenset({'shrimp', 'mineral water', 'chocolate'})  =  57\n",
      "frozenset({'shrimp', 'mineral water', 'ground beef'})  =  38\n",
      "frozenset({'shrimp', 'chocolate', 'milk'})  =  41\n",
      "frozenset({'spaghetti', 'frozen vegetables', 'shrimp'})  =  45\n",
      "frozenset({'shrimp', 'frozen vegetables', 'chocolate'})  =  40\n",
      "frozenset({'shrimp', 'spaghetti', 'chocolate'})  =  48\n",
      "frozenset({'shrimp', 'spaghetti', 'ground beef'})  =  45\n"
     ]
    }
   ],
   "source": [
    "countitem = 0\n",
    "for key,val in final_c.items():\n",
    "  print(key,\" = \",val)\n",
    "  countitem+=1\n",
    "  if(countitem>10):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7501, 120)\n"
     ]
    }
   ],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records)\n",
    "df = pd.DataFrame(te_ary,columns = te.columns_)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      support                                      itemsets\n",
      "0    0.020397                                     (almonds)\n",
      "1    0.008932                           (antioxydant juice)\n",
      "2    0.033329                                     (avocado)\n",
      "3    0.008666                                       (bacon)\n",
      "4    0.010799                              (barbecue sauce)\n",
      "..        ...                                           ...\n",
      "720  0.007466              (spaghetti, mineral water, soup)\n",
      "721  0.009332          (spaghetti, tomatoes, mineral water)\n",
      "722  0.006399            (spaghetti, mineral water, turkey)\n",
      "723  0.006266  (whole wheat rice, spaghetti, mineral water)\n",
      "724  0.005066              (spaghetti, olive oil, pancakes)\n",
      "\n",
      "[725 rows x 2 columns]\n",
      "\n",
      " Time taken for Mining using Apriori =  1.328125\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "print(apriori(df,min_support = 0.005,use_colnames = True))\n",
    "time_taken = time.process_time() - start\n",
    "print(\"\\n Time taken for Mining using Apriori = \",time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 - Test drive any one implementation in (1) or (2) adopting a Vertical Transaction Database format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "records=[[100,400,500,700,800,900],[100,200,300,400,600,800,900],[300,500,600,700,800,900],[200,400],[100,800]]\n",
    "Database = {}\n",
    "for i in range(len(records)):\n",
    "  Database[\"T\" + str(i+1)] = records[i]\n",
    "\n",
    "Itemset = {}\n",
    "for i in range(len(records)):\n",
    "  for j in range(len(records[i])):\n",
    "    if(frozenset([records[i][j]]) not in Itemset):\n",
    "      Itemset[frozenset([records[i][j]])] = 1\n",
    "    else:\n",
    "      Itemset[frozenset([records[i][j]])] += 1\n",
    "\n",
    "Database_vdf = {}\n",
    "for key,val in Database.items():\n",
    "  for x in val:\n",
    "    if(frozenset([x]) not in Database_vdf):\n",
    "      Database_vdf[frozenset([x])] = frozenset([key])\n",
    "    else:\n",
    "      Database_vdf[frozenset([x])] = frozenset([key]).union(Database_vdf[frozenset([x])])\n",
    "\n",
    "records_vdf = []\n",
    "for key,val in Database_vdf.items():\n",
    "  records_vdf.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     100    200    300    400    500    600    700    800    900\n",
      "0   True  False  False   True   True  False   True   True   True\n",
      "1   True   True   True   True  False   True  False   True   True\n",
      "2  False  False   True  False   True   True   True   True   True\n",
      "3  False   True  False   True  False  False  False  False  False\n",
      "4   True  False  False  False  False  False  False   True  False\n"
     ]
    }
   ],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      T1     T2     T3     T4     T5\n",
      "0   True   True  False  False   True\n",
      "1   True   True  False   True  False\n",
      "2   True  False   True  False  False\n",
      "3   True  False   True  False  False\n",
      "4   True   True   True  False   True\n",
      "5   True   True   True  False  False\n",
      "6  False   True  False   True  False\n",
      "7  False   True   True  False  False\n",
      "8  False   True   True  False  False\n"
     ]
    }
   ],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records_vdf).transform(records_vdf)\n",
    "df_vdf = pd.DataFrame(te_ary,columns=te.columns_)\n",
    "print(df_vdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     support                             itemsets\n",
      "0        0.6                                (100)\n",
      "1        0.4                                (200)\n",
      "2        0.4                                (300)\n",
      "3        0.6                                (400)\n",
      "4        0.4                                (500)\n",
      "..       ...                                  ...\n",
      "206      0.2       (800, 100, 900, 300, 400, 600)\n",
      "207      0.2       (800, 100, 900, 400, 500, 700)\n",
      "208      0.2       (800, 900, 200, 300, 400, 600)\n",
      "209      0.2       (800, 900, 300, 500, 600, 700)\n",
      "210      0.2  (800, 100, 900, 200, 300, 400, 600)\n",
      "\n",
      "[211 rows x 2 columns]\n",
      "Time Taken normally =  0.03125  seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "print(apriori(df,min_support=0.01,use_colnames=True))\n",
    "time_taken = time.process_time() - start\n",
    "print(\"Time Taken normally = \",str(time_taken),\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     support          itemsets\n",
      "0   0.666667              (T1)\n",
      "1   0.777778              (T2)\n",
      "2   0.666667              (T3)\n",
      "3   0.222222              (T4)\n",
      "4   0.222222              (T5)\n",
      "5   0.444444          (T2, T1)\n",
      "6   0.444444          (T3, T1)\n",
      "7   0.111111          (T4, T1)\n",
      "8   0.222222          (T5, T1)\n",
      "9   0.444444          (T3, T2)\n",
      "10  0.222222          (T2, T4)\n",
      "11  0.222222          (T5, T2)\n",
      "12  0.111111          (T3, T5)\n",
      "13  0.222222      (T3, T2, T1)\n",
      "14  0.111111      (T4, T2, T1)\n",
      "15  0.222222      (T5, T2, T1)\n",
      "16  0.111111      (T3, T5, T1)\n",
      "17  0.111111      (T3, T5, T2)\n",
      "18  0.111111  (T3, T5, T2, T1)\n",
      "Time Taken using Vertical Database Format =  0.015625  seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "print(apriori(df_vdf,min_support=0.01,use_colnames=True))\n",
    "time_taken = time.process_time() - start\n",
    "print(\"Time Taken using Vertical Database Format = \",str(time_taken),\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3 - Using a vertical transaction database notation, generate the FIâ€™s following the intersection approach (basic ECLAT) discussed in the class. Use earlier benchmark datasets in (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shrimp', 'almonds', 'avocado', 'vegetables mix', 'green grapes', 'whole weat flour', 'yams', 'cottage cheese', 'energy drink', 'tomato juice', 'low fat yogurt', 'green tea', 'honey', 'salad', 'mineral water', 'salmon', 'antioxydant juice', 'frozen smoothie', 'spinach', 'olive oil']\n"
     ]
    }
   ],
   "source": [
    "records = [[y for y in x if pd.notna(y)] for x in market_df.values.tolist()]\n",
    "print(records[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Database={}\n",
    "for i in range(len(records)):\n",
    "    Database[\"T\"+str(i+1)]=records[i]\n",
    "\n",
    "Database_vdf={}\n",
    "for key,val in Database.items():\n",
    "    for x in val:\n",
    "        if(frozenset([x]) not in Database_vdf):\n",
    "            Database_vdf[frozenset([x])]=frozenset([key])\n",
    "        else:\n",
    "            Database_vdf[frozenset([x])]=frozenset([key]).union(Database_vdf[frozenset([x])])\n",
    "\n",
    "records_vdf=[]\n",
    "for key,val in Database_vdf.items():\n",
    "    records_vdf.append(val)\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records_vdf).transform(records_vdf)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "Database_vdf={}\n",
    "for key,val in Database.items():\n",
    "    for x in val:\n",
    "        if(frozenset([x]) not in Database_vdf):\n",
    "            Database_vdf[frozenset([x])]=frozenset([key])\n",
    "        else:\n",
    "            Database_vdf[frozenset([x])]=frozenset([key]).union(Database_vdf[frozenset([x])])\n",
    "            \n",
    "def remove_items_vdf(Database_vdf,Min_Sup):\n",
    "    rem_keys=[]\n",
    "    for key,val in Database_vdf.items():\n",
    "        if(len(val)<Min_Sup):\n",
    "            rem_keys.append(key)\n",
    "    for key in rem_keys:\n",
    "        Database_vdf.pop(key)\n",
    "    return Database_vdf\n",
    "\n",
    "def get_vdf(Li,iteration):\n",
    "    New_Li={}\n",
    "    for key1,val1 in Li.items():\n",
    "        for key2,val2 in Li.items():\n",
    "            if(key1!=key2):\n",
    "                new_key=key1.union(key2)\n",
    "                if(len(new_key)>iteration):\n",
    "                    continue\n",
    "                else:\n",
    "                    New_Li[new_key]=val1.intersection(val2)\n",
    "    return New_Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Num  2\n",
      "Iteration Num  3\n",
      "Iteration Num  4\n",
      "\n",
      " Time Taken for Mining the itemset with min_support of 37.505 = 0.375 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "min_Sup_Count_vdf=0.005*len(records)\n",
    "\n",
    "Li=remove_items_vdf(Database_vdf,min_Sup_Count_vdf)\n",
    "iteration=1\n",
    "while(1):\n",
    "    iteration+=1\n",
    "    c=get_vdf(Li,iteration)\n",
    "    print(\"Iteration Num \",iteration)\n",
    "    Li=remove_items_vdf(c,min_Sup_Count_vdf)\n",
    "    if(len(Li)==0):\n",
    "        break\n",
    "    else:\n",
    "        final_vdf=Li\n",
    "\n",
    "time_taken=time.process_time() - start\n",
    "print(\"\\n Time Taken for Mining the itemset with min_support of \"+str(min_Sup_Count_vdf)+\" = \"+str(time_taken)+\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'mineral water', 'shrimp', 'eggs'})   frozenset({'T126', 'T108', 'T1327', 'T4095', 'T745', 'T2179', 'T2357', 'T237', 'T2008', 'T3528', 'T7468', 'T3869', 'T1214', 'T4925', 'T92', 'T667', 'T3616', 'T657', 'T3880', 'T6776', 'T478', 'T4993', 'T111', 'T1894', 'T976', 'T7475', 'T6973', 'T1817', 'T2262', 'T3696', 'T1726', 'T7370', 'T1059', 'T1608', 'T6101', 'T144', 'T1226', 'T143', 'T5062'}) \n",
      "\n",
      "frozenset({'mineral water', 'shrimp', 'milk'})   frozenset({'T126', 'T108', 'T5016', 'T745', 'T2179', 'T2242', 'T3629', 'T7265', 'T7344', 'T2125', 'T2796', 'T5466', 'T1605', 'T4933', 'T2198', 'T3242', 'T3260', 'T4121', 'T3869', 'T5219', 'T2156', 'T6022', 'T2105', 'T2633', 'T667', 'T3616', 'T789', 'T3481', 'T7131', 'T5746', 'T2065', 'T2359', 'T796', 'T2510', 'T809', 'T3041', 'T1606', 'T6157', 'T4993', 'T5639', 'T7475', 'T5019', 'T3755', 'T2430', 'T3643', 'T5698', 'T801', 'T5991', 'T3660', 'T504', 'T2335', 'T5792', 'T5277', 'T7370', 'T1059', 'T621', 'T2783', 'T5062', 'T6716'}) \n",
      "\n",
      "frozenset({'mineral water', 'frozen vegetables', 'shrimp'})   frozenset({'T126', 'T1038', 'T5607', 'T2179', 'T2357', 'T3447', 'T1605', 'T1366', 'T4933', 'T2198', 'T3579', 'T3242', 'T3981', 'T5219', 'T2668', 'T5460', 'T2820', 'T6022', 'T984', 'T2633', 'T4925', 'T2105', 'T3616', 'T5247', 'T5084', 'T3481', 'T2618', 'T1700', 'T3880', 'T7131', 'T5746', 'T6776', 'T472', 'T1606', 'T2173', 'T1894', 'T5785', 'T5639', 'T1959', 'T6973', 'T3643', 'T3660', 'T2335', 'T6523', 'T6101', 'T1993', 'T3370', 'T143', 'T3059', 'T2783', 'T1226', 'T1393', 'T700', 'T5062'}) \n",
      "\n",
      "frozenset({'spaghetti', 'mineral water', 'shrimp'})   frozenset({'T126', 'T1389', 'T2638', 'T1327', 'T4494', 'T4932', 'T5016', 'T4095', 'T2357', 'T5607', 'T676', 'T1657', 'T3528', 'T1605', 'T2198', 'T3579', 'T7468', 'T3242', 'T3981', 'T2668', 'T1214', 'T2820', 'T4925', 'T2633', 'T92', 'T2105', 'T4710', 'T2618', 'T4096', 'T6544', 'T2065', 'T2359', 'T796', 'T809', 'T462', 'T1606', 'T2173', 'T6554', 'T1894', 'T1346', 'T5639', 'T6973', 'T5019', 'T2430', 'T5855', 'T3723', 'T3643', 'T5991', 'T801', 'T3660', 'T504', 'T3696', 'T4914', 'T6523', 'T5479', 'T1059', 'T1993', 'T3693', 'T2783', 'T3059', 'T144', 'T4830', 'T2309', 'T6716'}) \n",
      "\n",
      "frozenset({'chocolate', 'mineral water', 'shrimp'})   frozenset({'T126', 'T4494', 'T5016', 'T4766', 'T2357', 'T745', 'T676', 'T3629', 'T2796', 'T1657', 'T3528', 'T1366', 'T2198', 'T3579', 'T7468', 'T3260', 'T3424', 'T1214', 'T5460', 'T2156', 'T6022', 'T2633', 'T5302', 'T3616', 'T1700', 'T1785', 'T2618', 'T5746', 'T472', 'T1606', 'T2173', 'T1894', 'T5639', 'T891', 'T7475', 'T5019', 'T2430', 'T3723', 'T3696', 'T3660', 'T1726', 'T6523', 'T5479', 'T5277', 'T2696', 'T1608', 'T621', 'T660', 'T3059', 'T2293', 'T4830', 'T1226', 'T1393', 'T2670', 'T143', 'T5062', 'T5792'}) \n",
      "\n",
      "frozenset({'ground beef', 'mineral water', 'shrimp'})   frozenset({'T2638', 'T1327', 'T4932', 'T4766', 'T5607', 'T2008', 'T7344', 'T2796', 'T3544', 'T4133', 'T1605', 'T3242', 'T3424', 'T3124', 'T3981', 'T2668', 'T2820', 'T2633', 'T5084', 'T4245', 'T2359', 'T2510', 'T462', 'T1606', 'T5395', 'T4993', 'T2173', 'T1894', 'T6973', 'T828', 'T3643', 'T5991', 'T3696', 'T4914', 'T6523', 'T3693', 'T143', 'T5792'}) \n",
      "\n",
      "frozenset({'chocolate', 'shrimp', 'milk'})   frozenset({'T126', 'T5016', 'T745', 'T3629', 'T2796', 'T1329', 'T3378', 'T2925', 'T2198', 'T469', 'T6439', 'T1835', 'T3260', 'T634', 'T2156', 'T6022', 'T2633', 'T4289', 'T3616', 'T1607', 'T5746', 'T4326', 'T1606', 'T3029', 'T5639', 'T7475', 'T1741', 'T5019', 'T2430', 'T566', 'T1375', 'T3195', 'T3660', 'T3686', 'T5277', 'T937', 'T621', 'T7324', 'T161', 'T5062', 'T5792'}) \n",
      "\n",
      "frozenset({'spaghetti', 'frozen vegetables', 'shrimp'})   frozenset({'T126', 'T5607', 'T4074', 'T2357', 'T1460', 'T480', 'T270', 'T1605', 'T2198', 'T3579', 'T3242', 'T3981', 'T2668', 'T2820', 'T4925', 'T2633', 'T2105', 'T2384', 'T2896', 'T2618', 'T3560', 'T4839', 'T1480', 'T3509', 'T1606', 'T2173', 'T3913', 'T727', 'T1894', 'T5639', 'T6578', 'T6973', 'T7331', 'T3643', 'T1717', 'T3660', 'T6523', 'T6280', 'T3085', 'T1993', 'T2783', 'T3269', 'T3059', 'T1784', 'T3864'}) \n",
      "\n",
      "frozenset({'chocolate', 'frozen vegetables', 'shrimp'})   frozenset({'T126', 'T2357', 'T6176', 'T6986', 'T1393', 'T4921', 'T3412', 'T3378', 'T1366', 'T2925', 'T2198', 'T3579', 'T469', 'T634', 'T5460', 'T6022', 'T2633', 'T3616', 'T1700', 'T2618', 'T5746', 'T472', 'T3509', 'T1606', 'T2173', 'T3029', 'T1894', 'T1886', 'T5639', 'T6578', 'T3660', 'T1399', 'T6523', 'T2079', 'T7324', 'T3059', 'T1226', 'T5400', 'T143', 'T5062'}) \n",
      "\n",
      "frozenset({'chocolate', 'spaghetti', 'shrimp'})   frozenset({'T126', 'T4494', 'T3006', 'T5016', 'T2357', 'T676', 'T1531', 'T4936', 'T5697', 'T1657', 'T772', 'T3528', 'T2198', 'T3579', 'T7468', 'T1214', 'T2320', 'T2633', 'T2618', 'T1941', 'T1702', 'T3448', 'T6060', 'T5487', 'T3509', 'T1606', 'T2173', 'T2734', 'T4988', 'T1894', 'T5639', 'T6578', 'T1741', 'T5019', 'T3576', 'T2430', 'T3723', 'T3195', 'T3696', 'T3660', 'T6523', 'T5479', 'T3059', 'T4998', 'T4830', 'T7482', 'T1471', 'T916'}) \n",
      "\n",
      "frozenset({'ground beef', 'spaghetti', 'shrimp'})   frozenset({'T2638', 'T1327', 'T4932', 'T4074', 'T5607', 'T1558', 'T1531', 'T6718', 'T5697', 'T5385', 'T1605', 'T3493', 'T3242', 'T1880', 'T3981', 'T2668', 'T2820', 'T2633', 'T2384', 'T3400', 'T4907', 'T3560', 'T2359', 'T2740', 'T462', 'T1606', 'T2173', 'T2734', 'T4988', 'T1894', 'T2382', 'T6086', 'T6973', 'T3576', 'T3643', 'T3195', 'T5991', 'T3696', 'T4914', 'T6523', 'T2811', 'T3085', 'T3693', 'T4998', 'T3864'}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "countitem=0\n",
    "for key,val in final_vdf.items():\n",
    "    print(key,\" \",val,\"\\n\")\n",
    "    countitem+=1\n",
    "    if(countitem>10):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4 - Extend the basic Apriori algorithm to generate Frequent Patterns which differentiate ab from ba (ordered patterns generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10: 'aabcacdcf', 20: 'adcabcae', 30: 'efabdfcb', 40: 'egafcbc'}\n"
     ]
    }
   ],
   "source": [
    "sequences={}\n",
    "sequences[10]=\"<a(abc)(ac)d(cf)>\"\n",
    "sequences[20]=\"<(ad)c(abc)(ae)>\"\n",
    "sequences[30]=\"<(ef)(ab)(df)cb>\"\n",
    "sequences[40]=\"<eg(af)cbc>\"\n",
    "\n",
    "for key,val in sequences.items():\n",
    "    sequences[key]=val.replace('<','').replace('(','').replace(')','').replace('>','')\n",
    "\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0={}\n",
    "for key in sequences.keys():\n",
    "    for x in sequences[key]:\n",
    "        if(x!='(' and x!=')' and x!='<' and x!='>'):\n",
    "            if(x not in c0):\n",
    "                c0[x]=1\n",
    "            else:\n",
    "                c0[x]+=1\n",
    "\n",
    "l0={}\n",
    "Min_Sup_Count=1\n",
    "for key in c0.keys():\n",
    "    if(c0[key]>=Min_Sup_Count):\n",
    "        l0[key]=c0[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No:  1\n",
      "Iteration No:  2\n",
      "Frequent Patterns are: ['ab', 'bc', 'ca']\n"
     ]
    }
   ],
   "source": [
    "def get_sequence(l0,sequences,count):\n",
    "    c1={}\n",
    "    for key1 in l0.keys():\n",
    "        for key2 in l0.keys():\n",
    "            if(key1!=key2):\n",
    "                new_key=key1+key2\n",
    "                if(len(new_key)==count):\n",
    "                    for key in sequences.keys():\n",
    "                        if(new_key not in c1):\n",
    "                            c1[new_key]=len(re.findall(new_key,sequences[key]))\n",
    "                        else:\n",
    "                            c1[new_key]+=len(re.findall(new_key,sequences[key]))\n",
    "    return c1\n",
    "                    \n",
    "                    \n",
    "def remove_sequence(c,Min_Sup_Count):\n",
    "    Li={}\n",
    "    for key in c.keys():\n",
    "        if(c[key]>=Min_Sup_Count):\n",
    "            Li[key]=c[key]\n",
    "    return Li\n",
    "\n",
    "sot=1\n",
    "final_sequence={}\n",
    "Min_Sup_Count=3\n",
    "while(1):\n",
    "    print(\"Iteration No: \",sot)\n",
    "\n",
    "    c=get_sequence(l0,sequences,sot+1)\n",
    "    \n",
    "    sot+=1\n",
    "    Li=remove_sequence(c,Min_Sup_Count)\n",
    "    if(len(Li)==0):\n",
    "        break\n",
    "    else:\n",
    "        final_sequence=Li\n",
    "\n",
    "print(\"Frequent Patterns are:\",[x for x in final_sequence.keys()] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5 - Implement following extensions to Apriori Algorithm (discussed / to be discussed in the class): Hash based strategy, Partitioning Approach. You may refer to online tutorials for a formal pseudocode description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T1': [['A', 'C'], ['C', 'D'], ['A', 'D']], 'T2': [['C', 'E'], ['B', 'C'], ['B', 'E']], 'T3': [['C', 'E'], ['B', 'C'], ['A', 'C'], ['B', 'E'], ['A', 'E'], ['A', 'B']], 'T4': [['B', 'E']]}\n"
     ]
    }
   ],
   "source": [
    "#HASH VARIANT\n",
    "transactions = [{'A','C','D'},{'B','C','E'},{'A','B','C','E'},{'B','E'}]\n",
    "\n",
    "database_hash={}\n",
    "count=0\n",
    "for i in transactions:\n",
    "    count+=1\n",
    "    database_hash[\"T\"+str(count)]=i\n",
    "\n",
    "c0={}\n",
    "for i in transactions:\n",
    "    for j in i:\n",
    "        if(j in c0):\n",
    "            c0[j]+=1\n",
    "        else:\n",
    "            c0[j]=1\n",
    "\n",
    "order={}\n",
    "count=0\n",
    "for key in sorted(c0.keys()):\n",
    "    count+=1\n",
    "    order[key]=count\n",
    "\n",
    "Min_Sup_Count=2\n",
    "rem_keys=[]\n",
    "Li={}\n",
    "for key,val in c0.items():\n",
    "    if(val>=Min_Sup_Count):\n",
    "        Li[key]=val\n",
    "        rem_keys.append(key)\n",
    "\n",
    "\n",
    "for key,val in database_hash.items():\n",
    "    val=[set(i) for i in itertools.combinations(val, 2)]\n",
    "    sets=[]\n",
    "    for i in range(len(val)):\n",
    "        sets.append(sorted(val[i]))\n",
    "\n",
    "    database_hash[key]=sets\n",
    "\n",
    "print(database_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{frozenset({'C', 'A'}): 3, frozenset({'C', 'B'}): 2, frozenset({'E', 'B'}): 3, frozenset({'C', 'E'}): 3}\n"
     ]
    }
   ],
   "source": [
    "Hash_Table={}\n",
    "for key,items in database_hash.items():\n",
    "    for x in items:\n",
    "        val=(order[x[0]]*10+order[x[1]])%7\n",
    "        if(val in Hash_Table):\n",
    "            Hash_Table[val].append(x)\n",
    "        else:\n",
    "            Hash_Table[val]=[x]\n",
    "\n",
    "C2={}\n",
    "keys=sorted(Li.keys())\n",
    "for i in range(len(keys)):\n",
    "    for j in range(i+1,len(keys)):\n",
    "        New_key=frozenset(set(keys[i]).union(set(keys[j])))\n",
    "        New_val=(order[keys[i]]*10+order[keys[j]])%7\n",
    "        C2[New_key]=len(Hash_Table[New_val])\n",
    "\n",
    "L2={}\n",
    "for key,val in C2.items():\n",
    "    if(val>=Min_Sup_Count):\n",
    "        L2[key]=val\n",
    "\n",
    "print(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       asparagus  almonds  antioxydant juice  asparagus  avocado  babies food  \\\n",
      "0          False     True               True      False     True        False   \n",
      "1          False    False              False      False    False        False   \n",
      "2          False    False              False      False    False        False   \n",
      "3          False    False              False      False     True        False   \n",
      "4          False    False              False      False    False        False   \n",
      "...          ...      ...                ...        ...      ...          ...   \n",
      "7496       False    False              False      False    False        False   \n",
      "7497       False    False              False      False    False        False   \n",
      "7498       False    False              False      False    False        False   \n",
      "7499       False    False              False      False    False        False   \n",
      "7500       False    False              False      False    False        False   \n",
      "\n",
      "      bacon  barbecue sauce  black tea  blueberries  ...  turkey  \\\n",
      "0     False           False      False        False  ...   False   \n",
      "1     False           False      False        False  ...   False   \n",
      "2     False           False      False        False  ...   False   \n",
      "3     False           False      False        False  ...    True   \n",
      "4     False           False      False        False  ...   False   \n",
      "...     ...             ...        ...          ...  ...     ...   \n",
      "7496  False           False      False        False  ...   False   \n",
      "7497  False           False      False        False  ...   False   \n",
      "7498  False           False      False        False  ...   False   \n",
      "7499  False           False      False        False  ...   False   \n",
      "7500  False           False      False        False  ...   False   \n",
      "\n",
      "      vegetables mix  water spray  white wine  whole weat flour  \\\n",
      "0               True        False       False              True   \n",
      "1              False        False       False             False   \n",
      "2              False        False       False             False   \n",
      "3              False        False       False             False   \n",
      "4              False        False       False             False   \n",
      "...              ...          ...         ...               ...   \n",
      "7496           False        False       False             False   \n",
      "7497           False        False       False             False   \n",
      "7498           False        False       False             False   \n",
      "7499           False        False       False             False   \n",
      "7500           False        False       False             False   \n",
      "\n",
      "      whole wheat pasta  whole wheat rice   yams  yogurt cake  zucchini  \n",
      "0                 False             False   True        False     False  \n",
      "1                 False             False  False        False     False  \n",
      "2                 False             False  False        False     False  \n",
      "3                 False             False  False        False     False  \n",
      "4                 False              True  False        False     False  \n",
      "...                 ...               ...    ...          ...       ...  \n",
      "7496              False             False  False        False     False  \n",
      "7497              False             False  False        False     False  \n",
      "7498              False             False  False        False     False  \n",
      "7499              False             False  False        False     False  \n",
      "7500              False             False  False         True     False  \n",
      "\n",
      "[7501 rows x 120 columns]\n"
     ]
    }
   ],
   "source": [
    "#PARTITIONING APPROACH\n",
    "records = [[y for y in x if pd.notna(y)] for x in market_df.values.tolist()]\n",
    "Database={}\n",
    "for i in range(len(records)):\n",
    "    Database[\"T\"+str(i+1)]=records[i]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(records).transform(records)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{frozenset({'chocolate'}): 1229.0, frozenset({'eggs'}): 1348.0, frozenset({'french fries'}): 1282.0, frozenset({'green tea'}): 991.0, frozenset({'milk'}): 972.0, frozenset({'mineral water'}): 1788.0, frozenset({'spaghetti'}): 1306.0}\n"
     ]
    }
   ],
   "source": [
    "def split_dfs(df,no):\n",
    "    dfs=[]\n",
    "    for i in range(0,df.shape[0],int(df.shape[0]/no)):\n",
    "        dfs.append(df.iloc[i:i+int(df.shape[0]/no)])\n",
    "    return dfs\n",
    "\n",
    "dfs=split_dfs(df,13)\n",
    "results=[]\n",
    "for i in dfs:\n",
    "    results.append(apriori(i, min_support=0.01,use_colnames=True))\n",
    "\n",
    "final_candidate_set={}\n",
    "for i in results:\n",
    "    for j in range(i.shape[0]):\n",
    "        item=i.iloc[j][1]\n",
    "        if(item in final_candidate_set):\n",
    "            final_candidate_set[item]+=(i.iloc[j][0]*int(df.shape[0]/13))\n",
    "        else:\n",
    "            final_candidate_set[item]=(i.iloc[j][0]*int(df.shape[0]/13))\n",
    "\n",
    "final_results={}\n",
    "Min_Sup_Count=int(df.shape[0]*(0.1))\n",
    "for key,val in final_candidate_set.items():\n",
    "    if(val>=Min_Sup_Count):\n",
    "        final_results[key]=val\n",
    "\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6 - Implement the Dynamic Itemset Counting Algorithm for Frequent Itemset Generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = [[1,1,0],[1,0,0],[0,1,1],[0,0,0]]\n",
    "unique_itemset =[{1},{2},{3}]\n",
    "min_supp = 1\n",
    "M = 2\n",
    "size = len(database)\n",
    "\n",
    "def get_subset(S,n):\n",
    "    a = itertools.combinations(S,n)\n",
    "    results = []\n",
    "    for i in a:\n",
    "        results.append(set(i))\n",
    "    return(results)\n",
    "\n",
    "def get_superset(S,unique_itemset):\n",
    "    #print(S)\n",
    "    result = []\n",
    "    a = set()\n",
    "    for i in unique_itemset:\n",
    "        if i.intersection(S)==set():\n",
    "            a = i.union(S)\n",
    "            result.append(a)\n",
    "            a = set()\n",
    "\n",
    "    return(result)\n",
    "\n",
    "def check_subset(Set,frequent_set):\n",
    "    subset = get_subset(Set,len(Set)-1)\n",
    "    flag = 1\n",
    "    temp = []\n",
    "\n",
    "    for i in frequent_set:\n",
    "        temp.append(i[0])\n",
    "\n",
    "    frequent_set = temp\n",
    "    for i in subset:\n",
    "        if i not in frequent_set:\n",
    "            flag=0\n",
    "            break\n",
    "\n",
    "    if flag:\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "def get_itemset(T):\n",
    "    result = set()\n",
    "    for i in range(len(T)):\n",
    "        if T[i]!=0:\n",
    "            result.add(i+1)\n",
    "\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DC: [[{1}, 0, 0], [{2}, 0, 0], [{3}, 0, 0]] \n",
      "\n",
      "Transaction : {1, 2}\n",
      "Transaction : {1}\n",
      "DS:  [[{1}, 2, 2], [{2}, 1, 2]]\n",
      "DC:  [[{3}, 0, 2], [{1, 2}, 0, 0]]\n",
      "SS:  []\n",
      "SC:  [] \n",
      "\n",
      "Transaction : {2, 3}\n",
      "Transaction : set()\n",
      "DS:  []\n",
      "DC:  [[{1, 2}, 0, 2], [{1, 3}, 0, 0], [{2, 3}, 0, 0]]\n",
      "SS:  [[{1}, 2, 4], [{2}, 2, 4], [{3}, 1, 4]]\n",
      "SC:  [] \n",
      "\n",
      "Transaction : {1, 2}\n",
      "Transaction : {1}\n",
      "DS:  []\n",
      "DC:  [[{1, 3}, 0, 2], [{2, 3}, 0, 2]]\n",
      "SS:  [[{1}, 2, 4], [{2}, 2, 4], [{3}, 1, 4], [{1, 2}, 1, 4]]\n",
      "SC:  [] \n",
      "\n",
      "Transaction : {2, 3}\n",
      "Transaction : set()\n",
      "DS:  []\n",
      "DC:  []\n",
      "SS:  [[{1}, 2, 4], [{2}, 2, 4], [{3}, 1, 4], [{1, 2}, 1, 4], [{2, 3}, 1, 4]]\n",
      "SC:  [[{1, 3}, 0, 4]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DC = []\n",
    "DS = []\n",
    "SC = []\n",
    "SS = []\n",
    "\n",
    "for i in unique_itemset:\n",
    "    DC.append([i,0,0])\n",
    "\n",
    "print(\"Initial DC:\",DC,\"\\n\")\n",
    "\n",
    "counter = 0\n",
    "T = []\n",
    "while len(DC)!=0 or len(DS)!=0:\n",
    "\n",
    "    for i in range(counter,counter+M):\n",
    "        index = i%size\n",
    "        T = get_itemset(database[index])\n",
    "        print(\"Transaction :\",T)\n",
    "\n",
    "        for item in DC:\n",
    "            item[2]+=1\n",
    "            if item[0].issubset(T):\n",
    "                item[1]+=1\n",
    "        for item in DS:\n",
    "            item[2]+=1\n",
    "            if item[0].issubset(T):\n",
    "                item[1]+=1\n",
    "\n",
    "    for item in copy.copy(DC):\n",
    "        if(item[1]>=min_supp):\n",
    "            DS.append(item)\n",
    "            DC.remove(item)\n",
    "\n",
    "    for item in copy.copy(DS):\n",
    "        if(item[2]==size):\n",
    "            SS.append(item)\n",
    "            DS.remove(item)\n",
    "    for item in copy.copy(DC):\n",
    "        if(item[2]==size):\n",
    "            SC.append(item)\n",
    "            DC.remove(item)\n",
    "\n",
    "    frequent_set = copy.copy(DS)\n",
    "    frequent_set.extend(SS)\n",
    "    for item in frequent_set:\n",
    "        S = get_superset(item[0],unique_itemset)\n",
    "        for i in S:\n",
    "            if (check_subset(i,frequent_set)):\n",
    "                flag=1\n",
    "                for x in DC:\n",
    "                    if x[0]==i:\n",
    "                        flag=0\n",
    "                for x in DS:\n",
    "                    if x[0]==i:\n",
    "                        flag=0\n",
    "                for x in SC:\n",
    "                    if x[0]==i:\n",
    "                        flag=0\n",
    "                for x in SS:\n",
    "                    if x[0]==i:\n",
    "                        flag=0\n",
    "                if flag:\n",
    "                    DC.append([i,0,0])\n",
    "\n",
    "    counter+=M\n",
    "    print(\"DS: \",DS)\n",
    "    print(\"DC: \",DC)\n",
    "    print(\"SS: \",SS)\n",
    "    print(\"SC: \",SC,\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a12d694c3ccac42055183a0ad11e659c6a2db5c6555ad2c8919d5814fd4e404f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
